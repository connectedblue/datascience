}
cr<-corr(threshold = 150)
rm(list=ls())
cr<-corr()
corr<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", threshold=0) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in 1:332) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
correlations<-integer(0) # empty result vector
for (d in filedata){       # go through each file
d<-d[complete.cases(d),] # filter out observations with NA
d<-d[c("sulfate", "nitrate", "ID")] # only keep the columns we're interested in
if (nrow(d)>threshold) {
#print(d[[3]])
#print(d[[1]],d[[2]])
correlations<-c(correlations, round(cor(d[[1]], d[[2]]), digits=5))
}
}
correlations
}
cr<-corr()
cr<-corr(threshold = 150)
head(cr)
cr<-corr(threshold = 20)
cr<-corr(threshold = 41)
cr<-corr(threshold = 40)
cr<-corr(threshold = 39)
pollutantmean<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", pollutant, id=1:332) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in id) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
result<-filedata[[1]][[pollutant]] # extract the values for the pollutant column from the first file
for (d in filedata[-1]){
result<-c(result, d[[pollutant]])  # append all the pollutant values from the remaining files onto the result variable
}
# calculate and return the mean of the result variable
mean(result, na.rm = TRUE)
}
x<-pollutantmean(pollutant = "nitrate")
x<-pollutantmean(pollutant = "nitrate", id=288)
x<-pollutantmean(pollutant = "nitrate", id=286:287)
x<-pollutantmean(pollutant = "nitrate", id=1:10)
x<-pollutantmean(pollutant = "sulfate", id=1:10)
x<-pollutantmean(pollutant = "sulfate", id=288)
x<-pollutantmean(pollutant = "nitrate", id=288)
x<-pollutantmean(pollutant = "nitrate", id=287)
x<-pollutantmean(pollutant = "nitrate", id=286)
x<-pollutantmean(pollutant = "sulfate", id=286)
x<-pollutantmean(pollutant = "nitrate", id=286)
x<-pollutantmean(pollutant = "nitrate")
a=c(1,2,"a")
a
a[is.numeric(a)]
is.numeric(a)
pollutantmean<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", pollutant, id=1:332) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in id) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
result<-filedata[[1]][[pollutant]] # extract the values for the pollutant column from the first file
for (d in filedata[-1]){
result<-c(result, d[[pollutant]])  # append all the pollutant values from the remaining files onto the result variable
}
# calculate and return the mean of the result variable
result
#mean(result, na.rm = TRUE)
}
x<-pollutantmean(pollutant = "nitrate")
x<-pollutantmean(pollutant = "nitrate", id=1:100)
x<-c("1", "2", "3")
y<-as.numeric(x)
pollutantmean<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", pollutant, id=1:332) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in id) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
result<-filedata[[1]][[pollutant]] # extract the values for the pollutant column from the first file
for (d in filedata[-1]){
result<-c(result, d[[pollutant]])  # append all the pollutant values from the remaining files onto the result variable
}
# calculate and return the mean of the result variable
result<-as.numeric(result)
result
#mean(result, na.rm = TRUE)
}
x<-pollutantmean(pollutant = "nitrate")
pollutantmean<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", pollutant, id=1:332) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in id) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
result<-filedata[[1]][[pollutant]] # extract the values for the pollutant column from the first file
for (d in filedata[-1]){
result<-c(result, d[[pollutant]])  # append all the pollutant values from the remaining files onto the result variable
}
# calculate and return the mean of the result variable
result<-as.numeric(result)
#result
mean(result, na.rm = TRUE)
}
x<-pollutantmean(pollutant = "nitrate")
corr<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", threshold=0) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in 1:332) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
correlations<-integer(0) # empty result vector
for (d in filedata){       # go through each file
d<-d[complete.cases(d),] # filter out observations with NA
d<-d[c("sulfate", "nitrate", "ID")] # only keep the columns we're interested in
if (nrow(d)>threshold) {
#print(d[[3]])
#print(d[[1]],d[[2]])
correlations<-c(correlations, round(cor(as.numeric(d[[1]]), as.numeric(d[[2]])), digits=5))
}
}
correlations
}
cr<-corr()
summary(cr)
cr<-corr(threshold = 150)
head(cr)
rm(list=ls())
complete<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", id=1:332) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in id) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
#
result<-data.frame(id=integer(0), nobs=integer(0)) # result data frame to return
for (d in filedata){       # go through each file
d<-d[complete.cases(d),] # filter out observations with NA
nobs<-nrow(d)            # how many clean rows
id=d[[1,4]]              # extract id of file
result[nrow(result)+1,]<-c(id,nobs)  # append to result data frame
}
# calculate and return the result variable
result
}
pollutantmean<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", pollutant, id=1:332) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in id) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
result<-filedata[[1]][[pollutant]] # extract the values for the pollutant column from the first file
for (d in filedata[-1]){
result<-c(result, d[[pollutant]])  # append all the pollutant values from the remaining files onto the result variable
}
# calculate and return the mean of the result variable
result<-as.numeric(result)
#result
mean(result, na.rm = TRUE)
}
corr<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", threshold=0) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in 1:332) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
correlations<-integer(0) # empty result vector
for (d in filedata){       # go through each file
d<-d[complete.cases(d),] # filter out observations with NA
d<-d[c("sulfate", "nitrate", "ID")] # only keep the columns we're interested in
if (nrow(d)>threshold) {
#print(d[[3]])
#print(d[[1]],d[[2]])
correlations<-c(correlations, round(cor(as.numeric(d[[1]]), as.numeric(d[[2]])), digits=5))
}
}
correlations
}
pollutantmean(pollutant = "sulphate", id=1:10)
pollutantmean(pollutant = "sulfate", id=1:10)
pollutantmean(pollutant = "nitrate", id=70:72)
pollutantmean(pollutant = "sulfate", id=34)
pollutantmean(pollutant = "nitrate")
cc <- complete(id=c(6, 10, 20, 34, 100, 200, 310))
print(cc$nobs)
cc <- complete(id=54)
print(cc$nobs)
set.seed(42)
cc <- complete(id=332:1)
cc <- complete()
cc <- complete(id=288)
complete<-function(directory="C:\\Users\\cs\\Dropbox\\Connected Blue\\Training\\data science\\git\\datascience\\R-Programming\\week 2\\rprog-data-specdata\\specdata", id=1:332) {
# Prepare to read in files from directory
library(readr)
files=list()
for (i in id) {
fn<-paste(sprintf("%03d", i),".csv",sep="")
files<-c(files,file.path(directory, fn, fsep = "\\"))
} # creates a list of files to read
filedata <- lapply(files, read_csv) # Puts all the files into filedata variable
#
result<-data.frame(id=integer(0), nobs=integer(0)) # result data frame to return
for (d in filedata){       # go through each file
d<-d[complete.cases(d),] # filter out observations with NA
nobs<-nrow(d)            # how many clean rows
if (nobs>0){
id=d[[1,4]]              # extract id of file
}
result[nrow(result)+1,]<-c(id,nobs)  # append to result data frame
}
# calculate and return the result variable
result
}
cc <- complete(id=332:1)
use <- sample(332, 10)
print(cc[use, "nobs"])
cr<-corr()
cr <- sort(cr)
set.seed(868)
out <- round(cr[sample(length(cr), 5)], 4)
print(out)
cr<-corr(threshold = 129)
cr <- sort(cr)
n <- length(cr)
set.seed(197)
out <- c(n, round(cr[sample(n, 5)], 4))
print(out)
cr<-corr(threshold = 2000)
n <- length(cr)
cr <- corr("specdata", 1000)
cr <- sort(cr)
print(c(n, round(cr, 4)))
cr<-corr(threshold = 2000)
n <- length(cr)
cr<-corr(threshold = 1000)
cr <- sort(cr)
print(c(n, round(cr, 4)))
m<-matrix(1:4,2,2)
m
solve(m)
m_inv=solve(m)
m_inv %*% m
## This function allows a special CacheMatrix to be created
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
x <<- y
inv <<- NULL
}
get <- function() x
setinv <- function(i) inv <<- i
getinv <- function() inv
list(set = set, get = get,
setinv = setinv,
getinv = getinv)
}
## This function allows the inverse to be returned from the cache if it has already been calculated
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
inv <- x$getinv()
if(!is.null(inv)) {
message("inverse being retrieved from cache")
return(inv)
}
mat <- x$get()
inv <- solve(mat)
x$setinv(mat)
mat
}
a<-matrix(1:4,2,2)
a
a1<-makeCacheMatrix(a)
a1
cacheSolve(a1)
cacheSolve(a1)
a
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
inv <- x$getinv()
if(!is.null(inv)) {
message("inverse being retrieved from cache")
return(inv)
}
mat <- x$get()
inv <- solve(mat)
x$setinv(inv)
inv
}
cacheSolve(a1)
a1<-makeCacheMatrix(a)
cacheSolve(a1)
cacheSolve(a1)
rm(a1)
rm(aa)
rm(a)
a<-matrix(1:4,2,2)
a
a1<-makeCacheMatrix(a)
a %*% a1
a1
a1<-cacheSolve(a)
a
a1<-cacheSolve(a)
rm(a1,a)
a<-matrix(1:4,2,2)
a
a1<-makeCacheMatrix(a)
a1
a2<-cacheSolve(a1)
a2
a1 %*% a2
a %*% a2
a3<-cacheSolve(a1)
a<--NULL
getwd()
setwd("..")
getwd()
setwd("week3")
setwd("week 3")
getwd()
download.file("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.csv?accessType=DOWNLOAD", destfile="accident.csv", method = "curl")
download.file("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.csv?accessType=DOWNLOAD", destfile="accident.csv")
accident<-read.table("accident.csv")
accident<-read.table("accident.csv", sep = ",", header = TRUE)
View(accident)
download.file("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.xlsx?accessType=DOWNLOAD", destfile="accident.xlsx")
library(xlsx)
install.packages("xlsx")
library(xlsx)
library(xlsx)
acc-xl<-read.xlsx("accident.xlsx", sheetIndex = 1, header = TRUE)
accxl<-read.xlsx("accident.xlsx", sheetIndex = 1, header = TRUE)
download.file("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.xlsx?accessType=DOWNLOAD", destfile="accident.xlsx", mode='wb')
accxl<-read.xlsx("accident.xlsx", sheetIndex = 1, header = TRUE)
accxl<-read.xlsx("accident.xlsx", sheetIndex = 1, header = TRUE)
head(accxl)
library(xml)
library(XML)
install.packages("XML")
library(XML)
accxml<-xmlTreeParse("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.xml?accessType=DOWNLOAD", useInternal=TRUE)
accxml<-xmlTreeParse("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.xml", useInternal=TRUE)
accxml<-xmlTreeParse("https://www.w3schools.com/xml/simple.xml", useInternal=TRUE)
accxml<-xmlTreeParse("http://www.w3schools.com/xml/simple.xml", useInternal=TRUE)
head(accxml)
rootnode<-xmlRoot(accxml)
xmlName(rootnode)
names(rootnode)
rootnode[[1]]
rootnode[[1]][[3]]
xpathSApply(rootnode, xmlValue)
xpathSApply(rootnode, xmlValue)
xpathSApply(rootnode)
rootnode
xmlSApply(rootnode, xmlValue)
rootnode
xmlSApply(rootnode, "//price", xmlValue)
xmlSApply(rootnode, "//name", xmlValue)
xpathSApply(rootnode, "//name", xmlValue)
xpathSApply(rootnode, "//price", xmlValue)
cricket<-htmlTreeParse("http://www.bbc.co.uk/sport/live/cricket/34404834", useInternal=TRUE)
xpathSApply(cricket, "//div[@class="lx-stream-tweet__text"]", xmlValue)
xpathSApply(cricket, "//div[@class="lx\-stream\-tweet__text"]", xmlValue)
xpathSApply(cricket, "//div[@class='lx-stream-tweet__text'']", xmlValue)
xpathSApply(cricket, "//div[@class='lx-stream-tweet__text]", xmlValue)
xpathSApply(cricket, "//div[@class='lx-stream-tweet__text']", xmlValue)
install.packages(jsonlite)
install.packages("jsonlite")
library(jsonlite)
accjson<fromJSON("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.json?accessType=DOWNLOAD")
accjson<-fromJSON("https://opendata.bristol.gov.uk/api/views/ywdp-yt9y/rows.json?accessType=DOWNLOAD")
names(accjson)
class(accjson)
class(accjson$view)
head(accjson)
accjson[[1]][[2]]
accjson[[1]][[1]]
library(data.table)
install.packages("data.table")
library(data.table)
df<-data.frame(x=rnorm(9), y=rep(c("a", "b", "c"),each=3), z=rnorm(9))
df
dt<-data.table(x=rnorm(9), y=rep(c("a", "b", "c"),each=3), z=rnorm(9))
dt
tables()
dt[,1]
dt
dt[1]
dt[,1]
dt$x
dt$y
dt[dt$y=="a"]
dt[dt$y=="a",]
dt[,mean(x)]
dt[,mean(z)]
dt[,sum(z)]
dt[,table(y)]
dt[,w:=Z+1]
dt[,w:=z+1]
dt
dt[[1,3]]
dt[[1,3]]<-666
dt
dt[,m:={t1=x+10; t1^2}]
dt
8.4^2
dt1<-data.table(x=c("x", "y", "z"), y=200:202)
dt1
dt2<-data.table(x=c("x", "x", "z", "a"), y=200:203)
dt2
dt2<-data.table(x=c("x", "x", "z", "a"), y=300:303)
dt2
dt2<-data.table(x=c("x", "x", "z", "a"), z=300:303)
dt1
dt2
setkey(dt1,x)
setkey(dt2,x)
merge (dt1,dt2)
merge (dt2,dt1)
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
0
info()
bye()
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv", destfile = "comm.csv")
comm<-read.table("comm.csv", sep = ",")
View(comm)
comm<-read.table("comm.csv", sep = ",", header=TRUE)
summary(comm$RT)
summary(comm$VAL)
comm[comm$VAL==24]
comm[,comm$VAL==24]
count(comm$VAL==24)
sum(comm$VAL==24)
sum(comm$VAL==24, na.rm=TRUE)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", destfile = "gas.xlsx", mode="wb")
dat<-read.xlsx("gas.xlsx", sheetIndex = 1, colIndex = 7:15, rowIndex = 18:23)
dat
sum(dat$Zip*dat$Ext,na.rm=T)
dat<-read.xlsx("gas.xlsx", sheetIndex = 1, colIndex = 7:15, rowIndex = 18:24)
dat
sum(dat$Zip*dat$Ext,na.rm=T)
rest<-xmlTreeParse("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", useInternalNodes = TRUE)
rest<-xmlTreeParse("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", useInternalNodes = TRUE)
restroot<-xmlRoot(rest)
xmlName(restroot)
names(restroot)
restroot[[1]]
restroot[1]
restroot[[1]][[1]]
restroot[[1]][[2]]
xpathSApply(restroot, "//zipcode", xmlValue)
zip<-xpathSApply(restroot, "//zipcode", xmlValue)
sum(zip=="21231", na.rm=TRUE)
sum(zip=="21231", na.rm=TRUE)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv", destfile = "id.csv")
DT<fread("id.csv")
DT<-fread("id.csv")
View(DT)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
mean({DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15})
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time({mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)})
system.time({})
system.time({DT[,mean(pwgtp15),by=SEX]})
system.time(DT[,mean(pwgtp15),by=SEX])
system.time({mean(DT$pwgtp15,by=DT$SEX)})
mean(DT$pwgtp15,by=DT$SEX)
mean(DT$pwgtp15,by=DT$SEX)
tapply(DT$pwgtp15,DT$SEX,mean)
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
sapply(split(DT$pwgtp15,DT$SEX),mean)
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
DT[,mean(pwgtp15),by=SEX]
mean(DT$pwgtp15,by=DT$SEX)
tapply(DT$pwgtp15,DT$SEX,mean)
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
DT[,mean(pwgtp15),by=SEX]
